{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.46.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.2.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.9)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.25.9)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\stefan\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "[x] Couldn't link model to 'en'\n",
      "Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "permissions and try re-running the command as admin, or use a virtualenv. You\n",
      "can still import the model as a module and call its load() method, or create the\n",
      "symlink manually.\n",
      "C:\\Users\\Stefan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\en_core_web_sm\n",
      "-->\n",
      "C:\\Users\\Stefan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\spacy\\data\\en\n",
      "[!] Download successful but linking failed\n",
      "Creating a shortcut link for 'en' didn't work (maybe you don't have admin\n",
      "permissions?), but you can still load the model via its full package name: nlp =\n",
      "spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You do not have sufficient privilege to perform this operation.\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext import datasets\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter Sentiment Analysis Dataset\n",
    "Source: http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentSource</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>pos</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>neg</td>\n",
       "      <td>Sentiment140</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID Sentiment SentimentSource  \\\n",
       "0       1       neg    Sentiment140   \n",
       "1       2       neg    Sentiment140   \n",
       "2       3       pos    Sentiment140   \n",
       "3       4       neg    Sentiment140   \n",
       "4       5       neg    Sentiment140   \n",
       "\n",
       "                                       SentimentText  \n",
       "0                       is so sad for my APL frie...  \n",
       "1                     I missed the New Moon trail...  \n",
       "2                            omg its already 7:30 :O  \n",
       "3  .. Omgaga. Im sooo  im gunna CRy. I've been at...  \n",
       "4           i think mi bf is cheating on me!!!   ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('datasets/tweets/tweets.csv', error_bad_lines = False)\n",
    "\n",
    "#tweets = tweets.head(50000)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataframe consists of 4 columns and we want to use only ‘Sentiment’ and ‘SentimentText’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pos</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I've been at...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                      SentimentText\n",
       "0       neg                       is so sad for my APL frie...\n",
       "1       neg                     I missed the New Moon trail...\n",
       "2       pos                            omg its already 7:30 :O\n",
       "3       neg  .. Omgaga. Im sooo  im gunna CRy. I've been at...\n",
       "4       neg           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets  = tweets.drop(columns = ['ItemID', 'SentimentSource'], axis = 1)\n",
    "\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['neg', 'pos'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets['Sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    26921\n",
       "neg    23079\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 0, 'Labels')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "ax = sns.barplot(x=tweets.Sentiment.unique(), y=tweets.Sentiment.value_counts())\n",
    "\n",
    "ax.set(xlabel='Labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(tweets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(      Sentiment                                      SentimentText\n",
       " 0           pos  @amyrenea omg so am I lol I fell asleep when i...\n",
       " 1           neg               @Adrienne_Bailon I want a shout out \n",
       " 2           neg  @Anonymousboy03 Plans for school stuff &amp; a...\n",
       " 3           neg  ... has hit a writer's block .. am loosing my ...\n",
       " 4           neg  ... trying to find people I know! I`m bored, i...\n",
       " ...         ...                                                ...\n",
       " 39995       pos   #robotpickuplines are so funny. check them out. \n",
       " 39996       pos  @annyo84 awh thankss.  yeah, i understand what...\n",
       " 39997       pos  @AmbiguityX ohh you're in twin cities?  i luv ...\n",
       " 39998       neg   Dinara lost again in Roland Garros. Why the S...\n",
       " 39999       pos  *yawn* fucking time zones shit. I'm really sic...\n",
       " \n",
       " [40000 rows x 2 columns],\n",
       "      Sentiment                                      SentimentText\n",
       " 0          pos  @aimeesays aww i hope it does fly by because J...\n",
       " 1          neg  #dontyouhate when you JUST painted yur nails a...\n",
       " 2          neg  - @EvertB which one? http://bit.ly/10o8LW, htt...\n",
       " 3          pos  *shriek* Bee almost flew here from window. I'm...\n",
       " 4          pos  @Alyssa_Milano granted if we lose it is to a w...\n",
       " ...        ...                                                ...\n",
       " 9995       neg  @aisforamylynn you're a badass for having a ba...\n",
       " 9996       pos  @acts_rox  I'm not particular about it being f...\n",
       " 9997       pos                     @@j311stp and the same to you!\n",
       " 9998       pos  .@nanere Sheila I heart you!! That &quot;Holly...\n",
       " 9999       neg   not the same without a goodnight....hm. Wish ...\n",
       " \n",
       " [10000 rows x 2 columns])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.reset_index(drop=True), test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39087</th>\n",
       "      <td>pos</td>\n",
       "      <td>@amyrenea omg so am I lol I fell asleep when i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30893</th>\n",
       "      <td>neg</td>\n",
       "      <td>@Adrienne_Bailon I want a shout out</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45278</th>\n",
       "      <td>neg</td>\n",
       "      <td>@Anonymousboy03 Plans for school stuff &amp;amp; a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>neg</td>\n",
       "      <td>... has hit a writer's block .. am loosing my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13653</th>\n",
       "      <td>neg</td>\n",
       "      <td>... trying to find people I know! I`m bored, i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                      SentimentText\n",
       "39087       pos  @amyrenea omg so am I lol I fell asleep when i...\n",
       "30893       neg               @Adrienne_Bailon I want a shout out \n",
       "45278       neg  @Anonymousboy03 Plans for school stuff &amp; a...\n",
       "16398       neg  ... has hit a writer's block .. am loosing my ...\n",
       "13653       neg  ... trying to find people I know! I`m bored, i..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 2), (10000, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('datasets/tweets/train_tweets.csv', index=False)\n",
    "test.to_csv('datasets/tweets/test_tweets.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_tweets.csv\n",
      "train_tweets.csv\n",
      "tweets.csv\n"
     ]
    }
   ],
   "source": [
    "!ls datasets/tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### removing non alphanumeric character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_clean(text):\n",
    "    \n",
    "    text = re.sub(r'[^A-Za-z0-9]+', ' ', text) \n",
    "    text = re.sub(r'https?:/\\/\\S+', ' ', text) \n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  processing and tokenization, so that it can be converted into indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'tagger', 'ner'])\n",
    "\n",
    "def tokenizer(s): \n",
    "    return [w.text.lower() for w in nlp(tweet_clean(s))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pytorch', 'is', 'awesome']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"pytorch is #awesome!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize = tokenizer)\n",
    "\n",
    "LABEL = torchtext.data.LabelField(dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafields = [('Sentiment', LABEL), ('SentimentText', TEXT)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TabularDataset to read csv files and process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn, tst = torchtext.data.TabularDataset.splits(path = 'datasets/tweets/', \n",
    "                                                train = 'train_tweets.csv',\n",
    "                                                test = 'test_tweets.csv',    \n",
    "                                                format = 'csv',\n",
    "                                                skip_header = True,\n",
    "                                                fields = datafields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 40000\n",
      "Number of testing examples: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(trn)}')\n",
    "print(f'Number of testing examples: {len(tst)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'pos',\n",
       " 'SentimentText': ['amyrenea',\n",
       "  'omg',\n",
       "  'so',\n",
       "  'am',\n",
       "  'i',\n",
       "  'lol',\n",
       "  'i',\n",
       "  'fell',\n",
       "  'asleep',\n",
       "  'when',\n",
       "  'it',\n",
       "  'was',\n",
       "  'on',\n",
       "  'last',\n",
       "  'night',\n",
       "  'so',\n",
       "  'now',\n",
       "  'i',\n",
       "  'get',\n",
       "  'to',\n",
       "  'finish',\n",
       "  'it']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(trn.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Sentiment': 'pos',\n",
       " 'SentimentText': ['aimeesays',\n",
       "  'aww',\n",
       "  'i',\n",
       "  'hope',\n",
       "  'it',\n",
       "  'does',\n",
       "  'fly',\n",
       "  'by',\n",
       "  'because',\n",
       "  'jt',\n",
       "  'episodes',\n",
       "  'are',\n",
       "  'usually',\n",
       "  'really',\n",
       "  'good',\n",
       "  'and',\n",
       "  'it',\n",
       "  's',\n",
       "  'early',\n",
       "  'but',\n",
       "  'so',\n",
       "  'far',\n",
       "  'this',\n",
       "  'ep',\n",
       "  'hassn',\n",
       "  't',\n",
       "  'disappointed']}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(tst.examples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building vocabulary using glove.6B.100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(trn, max_size=25000,\n",
    "                 vectors=\"glove.6B.100d\",\n",
    "                 unk_init=torch.Tensor.normal_)\n",
    "\n",
    "LABEL.build_vocab(trn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 25644), ('the', 12219), ('to', 12111), ('you', 10723), ('a', 9197), ('it', 8440), ('and', 6889), ('my', 6208), ('quot', 5582), ('s', 5564), ('that', 5306), ('is', 5203), ('for', 4971), ('in', 4852), ('t', 4844), ('m', 4683), ('me', 4588), ('of', 4331), ('on', 3918), ('have', 3752), ('so', 3612), ('but', 3506), ('be', 2932), ('not', 2887), ('was', 2775), ('just', 2724), ('can', 2523), ('do', 2418), ('are', 2351), ('your', 2320), ('with', 2269), ('good', 2203), ('like', 2173), ('at', 2131), ('no', 2119), ('this', 2093), ('all', 2069), ('up', 2066), ('now', 2063), ('get', 2044), ('we', 1988), ('u', 1890), ('love', 1885), ('lol', 1864), ('too', 1826), ('what', 1760), ('out', 1742), ('know', 1664), ('nt', 1608), ('amp', 1539)]\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.freqs.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', '<pad>', 'i', 'the', 'to', 'you', 'a', 'it', 'and', 'my']\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.itos[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(None, {'pos': 0, 'neg': 1})\n"
     ]
    }
   ],
   "source": [
    "print(LABEL.vocab.stoi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data in batches (BucketIterator() dataloader - to group same length sentences )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
    "                                (trn, tst),\n",
    "                                batch_size = 64,\n",
    "                                sort_key=lambda x: len(x.SentimentText),\n",
    "                                sort_within_batch=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RNN architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, \n",
    "                 output_dim, n_layers, bidirectional, dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, \n",
    "                           bidirectional = bidirectional, dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        output, hidden = self.rnn(embedded)\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "       \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(TEXT.vocab)\n",
    "\n",
    "embedding_dim = 100\n",
    "\n",
    "hidden_dim = 20\n",
    "output_dim = 1\n",
    "\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_dim, \n",
    "            embedding_dim, \n",
    "            hidden_dim, \n",
    "            output_dim, \n",
    "            n_layers, \n",
    "            bidirectional, \n",
    "            dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(25002, 100)\n",
       "  (rnn): GRU(100, 20, num_layers=2, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=40, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the embeddings from the field's vocab, and check they're the correct size, [vocab size, embedding dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25002, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "replacing the initial weights of the embedding layer with the pre-trained embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2932, -1.4328, -0.0502,  ..., -0.4358,  0.0277, -1.2281],\n",
       "        [-0.2607, -0.3080,  0.7337,  ...,  0.4167,  0.5066, -1.4307],\n",
       "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
       "        ...,\n",
       "        [-0.6137,  0.3310,  0.1758,  ...,  1.4781,  0.1865, -1.5560],\n",
       "        [ 0.3671,  0.5979, -0.5116,  ...,  0.6914,  0.1968, -1.0107],\n",
       "        [ 1.2596, -0.9782,  1.5508,  ..., -0.8135,  1.0153, -1.3793]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0465,  0.6197,  0.5665,  ..., -0.3762, -0.0325,  0.8062],\n",
      "        ...,\n",
      "        [-0.6137,  0.3310,  0.1758,  ...,  1.4781,  0.1865, -1.5560],\n",
      "        [ 0.3671,  0.5979, -0.5116,  ...,  0.6914,  0.1968, -1.0107],\n",
      "        [ 1.2596, -0.9782,  1.5508,  ..., -0.8135,  1.0153, -1.3793]])\n"
     ]
    }
   ],
   "source": [
    "unk_idx = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "pad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model.embedding.weight.data[unk_idx] = torch.zeros(embedding_dim)\n",
    "model.embedding.weight.data[pad_idx] = torch.zeros(embedding_dim)\n",
    "\n",
    "print(model.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.SentimentText).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.Sentiment)\n",
    "        \n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (rounded_preds == batch.Sentiment).float() \n",
    "        \n",
    "        acc = correct.sum() / len(correct)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 01 | Train Loss: 0.634 | Train Acc: 63.20% |\n",
      "| Epoch: 02 | Train Loss: 0.536 | Train Acc: 73.62% |\n",
      "| Epoch: 03 | Train Loss: 0.491 | Train Acc: 76.36% |\n",
      "| Epoch: 04 | Train Loss: 0.466 | Train Acc: 78.35% |\n",
      "| Epoch: 05 | Train Loss: 0.442 | Train Acc: 79.62% |\n",
      "| Epoch: 06 | Train Loss: 0.421 | Train Acc: 80.88% |\n",
      "| Epoch: 07 | Train Loss: 0.405 | Train Acc: 81.87% |\n",
      "| Epoch: 08 | Train Loss: 0.389 | Train Acc: 82.82% |\n",
      "| Epoch: 09 | Train Loss: 0.374 | Train Acc: 83.70% |\n",
      "| Epoch: 10 | Train Loss: 0.359 | Train Acc: 84.41% |\n",
      "| Epoch: 11 | Train Loss: 0.348 | Train Acc: 84.98% |\n",
      "| Epoch: 12 | Train Loss: 0.337 | Train Acc: 85.66% |\n",
      "| Epoch: 13 | Train Loss: 0.326 | Train Acc: 86.30% |\n",
      "| Epoch: 14 | Train Loss: 0.311 | Train Acc: 87.06% |\n",
      "| Epoch: 15 | Train Loss: 0.303 | Train Acc: 87.22% |\n",
      "| Epoch: 16 | Train Loss: 0.292 | Train Acc: 87.79% |\n",
      "| Epoch: 17 | Train Loss: 0.281 | Train Acc: 88.20% |\n",
      "| Epoch: 18 | Train Loss: 0.273 | Train Acc: 88.68% |\n",
      "| Epoch: 19 | Train Loss: 0.263 | Train Acc: 89.33% |\n",
      "| Epoch: 20 | Train Loss: 0.255 | Train Acc: 89.61% |\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "     \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.584 | Test Acc: 75.28%\n"
     ]
    }
   ],
   "source": [
    "epoch_loss = 0\n",
    "epoch_acc = 0\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_iterator:\n",
    "\n",
    "        predictions = model(batch.SentimentText).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, batch.Sentiment)\n",
    "\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))\n",
    "        correct = (rounded_preds == batch.Sentiment).float() \n",
    "        \n",
    "        acc = correct.sum()/len(correct)\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "test_loss = epoch_loss / len(test_iterator)\n",
    "test_acc = epoch_acc / len(test_iterator)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'I hate that show' \n",
    "\n",
    "#Run again for \"That movie was really nice\"\n",
    "#Run again for \"I hate that show but recently it has been quite good\"\n",
    "#Run again for \"That movie was decent but kind of fizzled out towards the end\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = [tok.text for tok in nlp.tokenizer(sentence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed = [TEXT.vocab.stoi[t] for t in tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.LongTensor(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tensor.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.sigmoid(model(tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9284198880195618"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
